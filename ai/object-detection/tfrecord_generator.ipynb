{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"datasets\"\n",
    "tfrecords_dir = \"tfrecords\"\n",
    "train_images_dir = os.path.join(root_dir, \"train2017\")\n",
    "val_images_dir = os.path.join(root_dir, \"val2017\")\n",
    "train_annotation_file = os.path.join(root_dir, \"lvis_v1_train.json\")\n",
    "val_annotation_file = os.path.join(root_dir, \"lvis_v1_val.json\")\n",
    "\n",
    "train_images_url = \"http://images.cocodataset.org/zips/train2017.zip\"\n",
    "val_images_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "train_annotations_url = (\n",
    "    \"https://s3-us-west-2.amazonaws.com/dl.fbaipublicfiles.com/LVIS/lvis_v1_train.json.zip\"\n",
    ")\n",
    "val_annotations_url = (\n",
    "    \"https://s3-us-west-2.amazonaws.com/dl.fbaipublicfiles.com/LVIS/lvis_v1_val.json.zip\"\n",
    ")\n",
    "coco_annotations_url = (\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download image files\n",
    "if not os.path.exists(train_images_dir):\n",
    "    image_zip = tf.keras.utils.get_file(\n",
    "        \"images.zip\", cache_dir=os.path.abspath(\".\"), origin=train_images_url, extract=True,\n",
    "    )\n",
    "    os.remove(image_zip)\n",
    "if not os.path.exists(val_images_dir):\n",
    "    image_zip = tf.keras.utils.get_file(\n",
    "        \"images.zip\", cache_dir=os.path.abspath(\".\"), origin=val_images_url, extract=True,\n",
    "    )\n",
    "    os.remove(image_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LVIS dataset has been downloaded and extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Download caption annotation files\n",
    "if not os.path.exists(train_annotation_file):\n",
    "    annotation_zip = tf.keras.utils.get_file(\n",
    "        \"captions.zip\",\n",
    "        cache_dir=os.path.abspath(\".\"),\n",
    "        origin=train_annotations_url,\n",
    "        extract=True,\n",
    "    )\n",
    "    os.remove(annotation_zip)\n",
    "    \n",
    "if not os.path.exists(val_annotation_file):\n",
    "    annotation_zip = tf.keras.utils.get_file(\n",
    "        \"captions.zip\",\n",
    "        cache_dir=os.path.abspath(\".\"),\n",
    "        origin=val_annotations_url,\n",
    "        extract=True,\n",
    "    )\n",
    "    os.remove(annotation_zip)\n",
    "\n",
    "print(\"The LVIS dataset has been downloaded and extracted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: 1270141\n",
      "Number of validation images: 244707\n"
     ]
    }
   ],
   "source": [
    "with open(train_annotation_file, \"r\") as f:\n",
    "    train_annotations = json.load(f)[\"annotations\"]\n",
    "    \n",
    "with open(val_annotation_file, \"r\") as f:\n",
    "    val_annotations = json.load(f)[\"annotations\"] \n",
    "\n",
    "print(f\"Number of train images: {len(train_annotations)}\")\n",
    "print(f\"Number of validation images: {len(val_annotations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'area': 82.38,\n",
      " 'bbox': [46.97, 459.24, 16.92, 10.84],\n",
      " 'category_id': 911,\n",
      " 'id': 31,\n",
      " 'image_id': 356358,\n",
      " 'segmentation': [[48.16,\n",
      "                   466.38,\n",
      "                   50.4,\n",
      "                   466.51,\n",
      "                   52.26,\n",
      "                   466.51,\n",
      "                   54.37,\n",
      "                   464.93,\n",
      "                   56.09,\n",
      "                   464.0,\n",
      "                   57.02,\n",
      "                   462.02,\n",
      "                   56.88,\n",
      "                   459.24,\n",
      "                   58.87,\n",
      "                   460.17,\n",
      "                   60.72,\n",
      "                   461.49,\n",
      "                   62.44,\n",
      "                   462.81,\n",
      "                   63.89,\n",
      "                   464.13,\n",
      "                   63.5,\n",
      "                   466.78,\n",
      "                   60.45,\n",
      "                   467.83,\n",
      "                   58.21,\n",
      "                   468.1,\n",
      "                   56.75,\n",
      "                   468.76,\n",
      "                   54.64,\n",
      "                   469.69,\n",
      "                   52.12,\n",
      "                   470.08,\n",
      "                   49.61,\n",
      "                   469.95,\n",
      "                   47.5,\n",
      "                   469.82,\n",
      "                   46.97,\n",
      "                   468.1,\n",
      "                   48.16,\n",
      "                   466.38]]}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_annotations[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_samples = 1270141\n",
    "val_num_samples = 244707\n",
    "\n",
    "if not os.path.exists(tfrecords_dir):\n",
    "    os.makedirs(tfrecords_dir)  # creating TFRecords output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def create_example(image, classes_text, bbox):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    feature = {\n",
    "        \"image/encoded\": image_feature(image),\n",
    "        \"image/object/bbox/xmin\": float_feature(xmin),\n",
    "        \"image/object/bbox/ymin\": float_feature(ymin),\n",
    "        \"image/object/bbox/xmax\": float_feature(xmax),\n",
    "        \"image/object/bbox/ymax\": float_feature(ymax),\n",
    "        \"image/object/class/text\": bytes_feature(classes_text),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aerosol_can\n"
     ]
    }
   ],
   "source": [
    "class_map = {idx: name for idx, name in enumerate(\n",
    "        open(\"./data/lvis.names\").read().splitlines())}\n",
    "print(class_map[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since LVIS val instance includes train data, create val first\n",
    "from collections import defaultdict\n",
    "samples = defaultdict(list)\n",
    "for annots in val_annotations:\n",
    "    samples[annots[\"image_id\"]].append(annots)\n",
    "    \n",
    "from PIL import Image\n",
    "\n",
    "with tf.io.TFRecordWriter(tfrecords_dir + \"/LVIS_val.tfrecord\") as writer:\n",
    "    for img_id, sample in samples.items():\n",
    "        bbox = [[] for _ in range(4)]\n",
    "        classes_text = []\n",
    "        image_path = f\"{train_images_dir}/{img_id:012d}.jpg\"\n",
    "        if not os.path.exists(image_path):\n",
    "            image_path = f\"{val_images_dir}/{img_id:012d}.jpg\"\n",
    "        image = open(image_path, 'rb').read()\n",
    "        size = Image.open(image_path).size\n",
    "        for instance in sample:\n",
    "            width, height = size\n",
    "            x, y, w, h = instance[\"bbox\"]\n",
    "            xmin = x/width\n",
    "            ymin = y/height\n",
    "            xmax = (x + w)/width\n",
    "            ymax = (y + h)/height\n",
    "            w, h = w/width, h/height\n",
    "            if 0.05 < w < 1 and 0.05 < h < 1:\n",
    "                bbox[0].append(xmin)\n",
    "                bbox[1].append(ymin)\n",
    "                bbox[2].append(xmax)\n",
    "                bbox[3].append(ymax)\n",
    "                classes_text.append(class_map[instance[\"category_id\"]-1].encode('utf8'))\n",
    "        example = create_example(image, classes_text, np.array(bbox))\n",
    "        writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759\n"
     ]
    }
   ],
   "source": [
    "samples = defaultdict(list)\n",
    "for annots in val_annotations:\n",
    "    samples[annots[\"image_id\"]].append(annots)\n",
    "    \n",
    "max_num = 0\n",
    "for img_id, sample in samples.items():\n",
    "    if len(sample) >= 100:\n",
    "        max_num = max(max_num, len(sample))\n",
    "print(max_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train tfrecord file\n",
    "samples = defaultdict(list)\n",
    "for annots in train_annotations:\n",
    "    samples[annots[\"image_id\"]].append(annots)\n",
    "\n",
    "with tf.io.TFRecordWriter(tfrecords_dir + \"/LVIS_train.tfrecord\") as writer:\n",
    "    for img_id, sample in samples.items():\n",
    "        bbox = [[] for _ in range(4)]\n",
    "        classes_text = []\n",
    "        image_path = f\"{train_images_dir}/{img_id:012d}.jpg\"\n",
    "        image = open(image_path, 'rb').read()\n",
    "        size = Image.open(image_path).size\n",
    "        for instance in sample:\n",
    "            width, height = size\n",
    "            x, y, w, h = instance[\"bbox\"]\n",
    "            xmin = x/width\n",
    "            ymin = y/height\n",
    "            xmax = (x + w)/width\n",
    "            ymax = (y + h)/height\n",
    "            w, h = w/width, h/height\n",
    "            if 0.05 < w < 1 and 0.05 < h < 1:\n",
    "                bbox[0].append(xmin)\n",
    "                bbox[1].append(ymin)\n",
    "                bbox[2].append(xmax)\n",
    "                bbox[3].append(ymax)\n",
    "                classes_text.append(class_map[instance[\"category_id\"]-1].encode('utf8'))\n",
    "        example = create_example(image, classes_text, np.array(bbox))\n",
    "        writer.write(example.SerializeToString())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
